{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ad646b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\piyus\\Desktop\\unstructured\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "# unstructured\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# langchain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# load .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "489c579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning document : ./docs/EJ1172284.pdf\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract 168 elements\n"
     ]
    }
   ],
   "source": [
    "def partition_document(file_path: str):\n",
    "    \"\"\"Extract elements from PDF using unstructured\"\"\"\n",
    "    print(f\"Partitioning document : {file_path}\")\n",
    "\n",
    "    elements = partition_pdf(\n",
    "        filename=file_path,\n",
    "        strategy=\"hi_res\",\n",
    "        infer_table_structure=True,\n",
    "        extract_image_block_types=[\"Image\"],\n",
    "        extract_image_block_to_payload=True\n",
    "    )\n",
    "\n",
    "    print(f\"Extract {len(elements)} elements\")\n",
    "    return elements\n",
    "\n",
    "file_path=\"./docs/EJ1172284.pdf\"\n",
    "elements= partition_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[0].category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf06bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 images\n"
     ]
    }
   ],
   "source": [
    "# gather images\n",
    "images = [element for element in elements if element.category == \"Image\"]\n",
    "print(f\"Found {len(images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "303c4d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Image',\n",
       " 'element_id': '3900d9bcbb56ad0d4fc52b2e9eb12c22',\n",
       " 'text': '20  ',\n",
       " 'metadata': {'coordinates': {'points': ((np.float64(780.0),\n",
       "     np.float64(2191.333333333333)),\n",
       "    (np.float64(780.0), np.float64(2265.0)),\n",
       "    (np.float64(873.3333333333334), np.float64(2265.0)),\n",
       "    (np.float64(873.3333333333334), np.float64(2191.333333333333))),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1654,\n",
       "   'layout_height': 2339},\n",
       "  'last_modified': '2025-12-29T01:20:13',\n",
       "  'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 3,\n",
       "  'image_base64': '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABKAF0DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKy9d8RaX4bskutVufJjkcRxqqM7yOQSFVVBJPB6Cq3hzxfovioXI0q6Z5bVgs8EsTRSRE9MqwB5x1oA3aK5K7+JfhWxv5bWbUJMQS+RNcrbSNBFJnG1pQu0HPvx3rrFYMoZSCCMgjvQAtFFFABRRRQAUUUUAFFFFAEU7wQxtcXDRpHCC5kkIAQAcnJ6cZ5rzjwx5+r+IfFXju2iaGxubUWunFlw1wsSnMxHoWAC+1dH468KXnjHR4dNttZ/s6ETCS4U2/nLcKOiMNy/LnkjPNS6DoviPTrkDVPEdrqFisXlraxaWtvtPGCCHPAAIxigDlPC1nayfs8eU6q0c2k3EkpPOWIdix98859q6z4fyyz/Dvw7JMSZDp0GSep+QYP5Vzi/DXVLfSZ/Dln4peDwxM75tBZq06ROSWiWXd905PJUnBr0C0tYbGzgtLaMRwQRrFGg6KqjAH5CgCaiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA/9k=',\n",
       "  'image_mime_type': 'image/jpeg',\n",
       "  'file_directory': './docs',\n",
       "  'filename': 'EJ1172284.pdf'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in images:\n",
    "#     print(i.text)\n",
    "\n",
    "images[2].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5c3963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 tables\n"
     ]
    }
   ],
   "source": [
    "# tables\n",
    "tables = [element for element in elements if element.category == \"Table\"]\n",
    "print(f\"Found {len(tables)} tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebcbb107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " creating smart chunks...\n",
      "created 20 chunks\n"
     ]
    }
   ],
   "source": [
    "# chunking by titile \n",
    "def create_chucks_by_title(elements):\n",
    "    \"\"\"Create intelligent chunks using title-based strategy\"\"\"\n",
    "    print(\" creating smart chunks...\")\n",
    "\n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        max_characters=3000,\n",
    "        new_after_n_chars=2400,\n",
    "        combine_text_under_n_chars=500\n",
    "    )\n",
    "\n",
    "    print(f\"created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "chunks=create_chucks_by_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fe8f96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x25b1951a210>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x25b1950bed0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5].metadata.orig_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2df69d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given the importance attached to new technologies, and, in particular the potential role of mobile devices in autonomous language learning, the terms mobile learning and mobile devices (MobDs) need first to be explained. As for mobile learning, no single agreed-upon definition of the term exists in the literature (Oz, 2015). This is because some researchers define mobile learning as an extension of e-learning built upon mobile devices whereas some other researchers understand it as learning that happens anywhere and anytime (cf. Oz, 2015). As far as mobile devices are concerned, they can be defined as ‚Äúany device that is small, autonomous and unobtrusive enough to accompany us in every moment and can be used for educational purposes‚Äù (Trifanova Knapp, Ronchetti & Gamper, 2004, p. 3) or as ‚Äúhand held electronic devices that can be comfortably carried around in a pocket or bag, including MP3 players, digital recorders, e-readers, tablets, and smartphones‚Äù (Kukulska-Hulme, Norris & Donohue, 2015, p. 39).\\n\\nA lot of studies concerning the use of mobile technology and mobile devices in language learning have been published. The findings of these studies concentrated on, for example, language learners‚Äô views on the use of mobile devices in language instruction (e.g. Oz, 2015), students‚Äô attitudes towards using mobile phones as instructional tools for foreign language learning (e.g. Cakir, 2015), profiling mobile language learners (e.g. Byrne & Diem, 2014), their effect on learning a foreign/second language (e.g. Nah, White & Sussex, 2008; Cavus & Ibrahim, 2009; Zhang, Song & Burston, 2011), distance language learning (e.g. Demouy, Jones, Kan, Kukulska-Hulme & Eardley, 2016), informal language learning practices (Reinders & Cho, 2011; Jones, 2015), learners‚Äô use of mobile devices for learning a foreign language (Stockwell, 2007; Dashtestani, 2015) and autonomy in language learning (e.g. D√≠az-Vera, 2012; Djoub, 2015). In addition to this, researchers investigated a number of applications of mobile devices and presented both benefits and drawbacks of the usage of mobile technologies (e.g. Miangah & Nezarat, 2012), discussed the use of mobile devices in supporting social contacts and collaborative learning (e.g. Kukulska-Hulme & Shield, 2008) and offered guidelines related to the implementation of mobile learning into second/foreign language instruction (e.g. Kukulska-Hulme et al., 2015).'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_content_types(chunk):\n",
    "    \"\"\"Analyze what types of content are in a chunk\"\"\"\n",
    "    content_data = {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'types': ['text']\n",
    "    }\n",
    "    \n",
    "    # Check for tables and images in original elements\n",
    "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            element_type = type(element).__name__\n",
    "            \n",
    "            # Handle tables\n",
    "            if element_type == 'Table':\n",
    "                content_data['types'].append('table')\n",
    "                table_html = getattr(element.metadata, 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "            \n",
    "            # Handle images\n",
    "            elif element_type == 'Image':\n",
    "                if hasattr(element, 'metadata') and hasattr(element.metadata, 'image_base64'):\n",
    "                    content_data['types'].append('image')\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "    \n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data\n",
    "\n",
    "def create_ai_enhanced_summary(text: str, tables: List[str], images: List[str]) -> str:\n",
    "    \"\"\"Create AI-enhanced summary for mixed content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "\n",
    "        CONTENT TO ANALYZE:\n",
    "        TEXT CONTENT:\n",
    "        {text}\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add tables if present\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i, table in enumerate(tables):\n",
    "                prompt_text += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "        \n",
    "                prompt_text += \"\"\"\n",
    "                YOUR TASK:\n",
    "                Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "                1. Key facts, numbers, and data points from text and tables\n",
    "                2. Main topics and concepts discussed  \n",
    "                3. Questions this content could answer\n",
    "                4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "                5. Alternative search terms users might use\n",
    "\n",
    "                Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "                SEARCHABLE DESCRIPTION:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add images to the message\n",
    "        for image_base64 in images:\n",
    "            message_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "            })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå AI summary failed: {e}\")\n",
    "        # Fallback to simple summary\n",
    "        summary = f\"{text[:300]}...\"\n",
    "        if tables:\n",
    "            summary += f\" [Contains {len(tables)} table(s)]\"\n",
    "        if images:\n",
    "            summary += f\" [Contains {len(images)} image(s)]\"\n",
    "        return summary\n",
    "\n",
    "def summarise_chunks(chunks):\n",
    "    \"\"\"Process all chunks with AI Summaries\"\"\"\n",
    "    print(\"üß† Processing chunks with AI Summaries...\")\n",
    "    \n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk = i + 1\n",
    "        print(f\"   Processing chunk {current_chunk}/{total_chunks}\")\n",
    "        \n",
    "        # Analyze chunk content\n",
    "        content_data = separate_content_types(chunk)\n",
    "        \n",
    "        # Debug prints\n",
    "        print(f\"     Types found: {content_data['types']}\")\n",
    "        print(f\"     Tables: {len(content_data['tables'])}, Images: {len(content_data['images'])}\")\n",
    "        \n",
    "        # Create AI-enhanced summary if chunk has tables/images\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(f\"     ‚Üí Creating AI summary for mixed content...\")\n",
    "            try:\n",
    "                enhanced_content = create_ai_enhanced_summary(\n",
    "                    content_data['text'],\n",
    "                    content_data['tables'], \n",
    "                    content_data['images']\n",
    "                )\n",
    "                print(f\"     ‚Üí AI summary created successfully\")\n",
    "                print(f\"     ‚Üí Enhanced content preview: {enhanced_content[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå AI summary failed: {e}\")\n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(f\"     ‚Üí Using raw text (no tables/images)\")\n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        # Create LangChain Document with rich metadata\n",
    "        doc = Document(\n",
    "            page_content=enhanced_content,\n",
    "            metadata={\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": content_data['text'],\n",
    "                    \"tables_html\": content_data['tables'],\n",
    "                    \"images_base64\": content_data['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        langchain_documents.append(doc)\n",
    "    \n",
    "    print(f\"‚úÖ Processed {len(langchain_documents)} chunks\")\n",
    "    return langchain_documents\n",
    "\n",
    "\n",
    "# Process chunks with AI\n",
    "processed_chunks = summarise_chunks(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unstructured (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
